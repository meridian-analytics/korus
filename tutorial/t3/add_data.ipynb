{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6498e38",
   "metadata": {},
   "source": [
    "# Adding data to the database\n",
    "\n",
    "Korus stores acoustic annotations and metadata in an SQLite database. The data are organized in several cross-referenced tables. The main tables are,\n",
    "\n",
    " * deployment: metadata pertaining hydrophone deployments\n",
    " * file: metadata pertaining to audio files\n",
    " * taxonomy: taxonomies used for annotating the acoustic data\n",
    " * job: metadata pertaining to annotation jobs\n",
    " * annotation: acoustic annotations\n",
    " \n",
    "In this tutorial you will learn how to create a Korus database and add data to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d01827",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "* [Getting ready](#getting-ready)\n",
    "* [Creating a database](#creating-a-database)\n",
    "* [Adding a hydrophone deployment](#adding-a-hydrophone-deployment)\n",
    "* [Adding audio files](#adding-audio-files)\n",
    "* [Adding a taxonomy](#adding-a-taxonomy)\n",
    "* [Adding an annotation job](#adding-an-annotation-job)\n",
    "* [Linking files to jobs](#linking-files-to-jobs) \n",
    "* [Adding annotations](#adding-annotations)\n",
    "* [Customization](#customization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4fddc7",
   "metadata": {},
   "source": [
    "## Getting ready <a class=\"anchor\" id=\"getting-ready\"></a>\n",
    "We begin by importing the necessary modules, classes, functions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41a7b32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import korus.db as kdb\n",
    "from korus.util import collect_audiofile_metadata\n",
    "from korus.tax import AcousticTaxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64403c33",
   "metadata": {},
   "source": [
    "## Creating a database <a class=\"anchor\" id=\"creating-a-database\"></a>\n",
    "Creating an empty SQLite database with the Korus schema is straightforward. Just specify the desired path of the database file and call the `create_db` function. (If a file already already exists at the specified path, you will be asked if you want to overwrite it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd60097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_db = \"mydb.sqlite\"\n",
    "conn = kdb.create_db(path_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eda27f9",
   "metadata": {},
   "source": [
    "The call to `create_db` returns an open database connection, `conn`, which we will be using to add data to the database. Once we are done, we must remember to commit the changes with `conn.commit()` and finally closing the connection to the database with `conn.close()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d6cb0c",
   "metadata": {},
   "source": [
    "## Adding a hydrophone deployment <a class=\"anchor\" id=\"adding-a-hydrophone-deployment\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee6f62",
   "metadata": {},
   "source": [
    "Having created the database, the next step is usually to add a hydrophone deployment. The Korus schema allows you to enter various relevant metadata pertaining to a hydrophone deployment. For example, the time period during which the hydrophone was operating, the geographical location (latitude and longitude), and the deployment depth, to name but a few. \n",
    "\n",
    "To view all the metadata fields, you can do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59963ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('id', 'INTEGER'), ('name', 'TEXT'), ('owner', 'TEXT'), ('start_utc', 'TEXT'), ('end_utc', 'TEXT'), ('location', 'TEXT'), ('latitude_deg', 'REAL'), ('longitude_deg', 'REAL'), ('depth_m', 'REAL'), ('trajectory', 'JSON'), ('latitude_min_deg', 'REAL'), ('latitude_max_deg', 'REAL'), ('longitude_min_deg', 'REAL'), ('longitude_max_deg', 'REAL'), ('depth_min_m', 'REAL'), ('depth_max_m', 'REAL'), ('license', 'TEXT'), ('hydrophone', 'TEXT'), ('bits_per_sample', 'INTEGER'), ('sample_rate', 'INTEGER'), ('num_channels', 'INTEGER'), ('sensitivity', 'REAL'), ('comments', 'TEXT')]\n"
     ]
    }
   ],
   "source": [
    "cursor = conn.execute(\"SELECT name,type FROM PRAGMA_TABLE_INFO('deployment')\")\n",
    "columns = cursor.fetchall()\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3feafb6",
   "metadata": {},
   "source": [
    "This lists the names and the types of all the columns in the `deployment` table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7053ac22",
   "metadata": {},
   "source": [
    "While you should always try to enter data for as many metadata fields as possible, the majority of the fields are optional. In fact, the only metadata field required for submitting a hydrophone deployment to the database is the `name` field. (The `id` field is automatically populated.)\n",
    "\n",
    "Thus, using standard SQLite syntax, we can easily add a deployment to the database, providing only a descriptive name,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fae43226",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = conn.cursor()\n",
    "cursor = cursor.execute(\"INSERT INTO deployment (name) VALUES ('My First Hydrophone Deployment')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae4ea03",
   "metadata": {},
   "source": [
    "Now, let's add a more comprehensive set of metadata to the deployment table. This time, we will be using Korus's `insert_row` function to insert the data, which allows us to submit the data in the form of a dictionary, which can be convenient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b2e49f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the data in a dictionary\n",
    "data = {\n",
    "    \"owner\": \"Mr. Moose\",  # owner of the data, can be a person or an organisation\n",
    "    \"name\": \"Seaforth2020\",  # descriptive name for the deployment\n",
    "    \"start_utc\": \"2020-06-01\", # UTC start time of recording\n",
    "    \"end_utc\": \"2020-10-31\", # UTC end time of recording\n",
    "    \"location\": \"Seaforth, NS, Canada\", # descriptive name for the deployment location\n",
    "    \"latitude_deg\": 44.660784, # latitude, in degrees\n",
    "    \"longitude_deg\": -63.253176, # longitude, in degrees\n",
    "    \"depth_m\": 8.0, # depth, in meters\n",
    "    \"license\": \"CCBY\", # license/terms governing access to and use of data\n",
    "    \"hydrophone\": \"VeryFineInstruments Inc., SuperListener101\", # hydrophone make and model\n",
    "    \"bits_per_sample\": 24, # bits per sample\n",
    "    \"sample_rate\": 128000, # sampling rate in samples/s\n",
    "    \"num_channels\": 1, # no. channels\n",
    "    \"sensitivity\": -60, # calibrated sensitivity in ...\n",
    "    \"comments\": \"popular surfing spot on the Canadian east coast\" # additional comments\n",
    "}\n",
    "\n",
    "# submit it to the database\n",
    "cursor = kdb.insert_row(conn, table_name=\"deployment\", values=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48702983",
   "metadata": {},
   "source": [
    "Note that, although not used here, the deployment table also has columns for storing spatio-temporal trajectory coordinates - relevant for mobile hydrophones, e.g., when towed behind a vessel or mounted on a glider.\n",
    "\n",
    "The call to `insert_row` returns a cursor, just like the `execute` function above. Using this cursor, we can query the table using standard SQLite syntax to check that the data was correctly inserted. Here we obtain the names, UTC start times, and locations of all the entries in the deployment table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aacea9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My First Hydrophone Deployment', None, None), ('Seaforth2020', '2020-06-01', 'Seaforth, NS, Canada')]\n"
     ]
    }
   ],
   "source": [
    "cursor = cursor.execute(\"SELECT name, start_utc, location FROM deployment\")\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c83a42",
   "metadata": {},
   "source": [
    "Finally, make sure to commit the changes, and if you are done interacting with the database, close the connection to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33274554",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d235c934",
   "metadata": {},
   "source": [
    "## Adding audio files <a class=\"anchor\" id=\"adding-audio-files\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0c03f0",
   "metadata": {},
   "source": [
    "Having successfully added two hydrophone deployments to the database, let us now add some audio files to the database. Note that we are only adding information *about* the audio files (metadata) and not the acoustic data contained *within* the files. We begin by reconnecting to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b9e0c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(path_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d72b09d",
   "metadata": {},
   "source": [
    "Korus stores audio file metadata in a table named `file`. Let us inspect its columns,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0df0d3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('id', 'INTEGER'), ('deployment_id', 'INTEGER'), ('storage_id', 'INTEGER'), ('filename', 'TEXT'), ('relative_path', 'TEXT'), ('sample_rate', 'INTEGER'), ('num_samples', 'INTEGER'), ('downsample', 'TEXT'), ('format', 'TEXT'), ('codec', 'TEXT'), ('start_utc', 'TEXT'), ('end_utc', 'TEXT')]\n"
     ]
    }
   ],
   "source": [
    "cursor = conn.execute(\"SELECT name,type FROM PRAGMA_TABLE_INFO('file')\")\n",
    "columns = cursor.fetchall()\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a4c5bf",
   "metadata": {},
   "source": [
    "The first column (`id`) is the file unique ID. Every file added to the database is automatically assigned an ID, so this is not something we need to worry about. (SQLite uses integers as IDs, starting at 1.)\n",
    "\n",
    "The second and third columns (`deployment_id`, `storage_id`) are the ID of the hydrophone deployment that produced the audio file and the ID of the data-storage location. Note that these are required fields, meaning you will not be able to add an audio file without linking it to an (already existing) deployment and data-storage location in the database. \n",
    "\n",
    "You can use a query like this to view the IDs and names of all the deployments in the database,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70ff32d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'My First Hydrophone Deployment'), (2, 'Seaforth2020')]\n"
     ]
    }
   ],
   "source": [
    "cursor = cursor.execute(\"SELECT id, name FROM deployment\")\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1160c05",
   "metadata": {},
   "source": [
    "You are also required to specify the audio filename (`filename`) and the relative directory path (`relative_path`). The last two required fields are the sampling rate (`sample_rate`) and the number of samples (`num_samples`). The remaining fields are all optional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9744cb1",
   "metadata": {},
   "source": [
    "Let us try to add a single audio file to the database, linking it to the first deployment and providing only the minimum required metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6adac6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, add the data-storage location\n",
    "data = {\n",
    "    \"name\": \"My personal laptop\",\n",
    "    \"path\": \"/home/mrmoose/acoustic-data/\",\n",
    "    \"description\": \"All audio files recorded by Mr. Moose between 2016 and 2021\",\n",
    "}\n",
    "\n",
    "# now, add a single audio file\n",
    "data = {\n",
    "    \"deployment_id\": 1,\n",
    "    \"storage_id\": 1,\n",
    "    \"filename\": \"a-10-second-long-recording.wav\",\n",
    "    \"relative_path\": \"20200602\",\n",
    "    \"sample_rate\": 32000,  #sampling rate in samples/s\n",
    "    \"num_samples\": 320000  #file size, no. samples\n",
    "}\n",
    "\n",
    "cursor = kdb.insert_row(conn, table_name=\"file\", values=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0eefab",
   "metadata": {},
   "source": [
    "This works well for a single audio file, but if your dataset consists of many files entering metadata for file manually quickly becomes tedious. In such cases, you may want to automate the metadata collection. In particular, you may want to automatically\n",
    "\n",
    " * generate a list of all the audio files in a given directory\n",
    " * parse timestamps from the filenames\n",
    " * inspect the files to determine their sampling rate and size\n",
    " \n",
    "Fortunately, Korus has a function that does exactly this. Let's see how it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c83c45c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  format                   start_utc  num_samples  sample_rate                     end_utc                         filename relative_path\n",
      "0   FLAC  2013-06-23 08:00:00.116000      1800055         1000  2013-06-23 08:30:00.171000  audio_20130623T080000.116Z.flac      20130623\n",
      "1   FLAC  2013-06-23 08:30:00.117000      1800055         1000  2013-06-23 09:00:00.172000  audio_20130623T083000.117Z.flac      20130623\n",
      "2   FLAC  2013-06-24 08:00:00.118000      1800055         1000  2013-06-24 08:30:00.173000  audio_20130624T080000.118Z.flac      20130624\n"
     ]
    }
   ],
   "source": [
    "# define a function for parsing the timestamps embedded in the audio filenames\n",
    "# this function should take the filename as input (in the form of a string) and \n",
    "# return the UTC start time of the file (in the form of a datetime object)\n",
    "def timestamp_parser(x):\n",
    "    fmt = \"%Y%m%dT%H%M%S.%f\"\n",
    "    p = x.rfind(\".\")\n",
    "    s = x[p-20: p-1]\n",
    "    return datetime.strptime(s, fmt)\n",
    "\n",
    "# now let's collect metadata on all the FLAC files in the .data/ folder (including subfolders)\n",
    "df = collect_audiofile_metadata(\"./data\", \"FLAC\", timestamp_parser)\n",
    "\n",
    "# view the contents of the returned dataframe\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7a3a88",
   "metadata": {},
   "source": [
    "The `collect_audiofile_metadata` function returns the metadata in the form of a pandas DataFrame. Having reviewed the metadata, we are now ready to submit the data to the database. We do this one entry at the time,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "013c3f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _,row in df.iterrows(): #loop over all rows in the dataframe\n",
    "    data = row.to_dict() #convert pd.Series to dict\n",
    "\n",
    "    # additional required metadata\n",
    "    data[\"storage_id\"] = 1\n",
    "    data[\"deployment_id\"] = 2 \n",
    "    \n",
    "    # insert in the 'file' table\n",
    "    cursor = kdb.insert_row(conn, table_name=\"file\", values=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf942b5",
   "metadata": {},
   "source": [
    "Let's check that things look alright by querying for the deployment ID, filename, and UTC start times of all the audio files in the database,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6054b697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'a-10-second-long-recording.wav', None), (2, 'audio_20130623T080000.116Z.flac', '2013-06-23 08:00:00.116000'), (2, 'audio_20130623T083000.117Z.flac', '2013-06-23 08:30:00.117000'), (2, 'audio_20130624T080000.118Z.flac', '2013-06-24 08:00:00.118000')]\n"
     ]
    }
   ],
   "source": [
    "cursor = cursor.execute(\"SELECT deployment_id, filename, start_utc FROM file\")\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aefe75d",
   "metadata": {},
   "source": [
    "Happy with the changes made, we commit them to the database and close the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "469314a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f2de34",
   "metadata": {},
   "source": [
    "## Adding a taxonomy <a class=\"anchor\" id=\"adding-a-taxonomy\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea778228",
   "metadata": {},
   "source": [
    "As always, we begin by reconnecting to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f0f76a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(path_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170d33b0",
   "metadata": {},
   "source": [
    "Next, we import the annotation taxonomy that we created in the 1st tutorial,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ade50ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7d20809c5dc0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdb.import_taxonomy(conn, src=\"tax_t1.sqlite\", name=\"my-first-taxonomy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df600f2",
   "metadata": {},
   "source": [
    "To verify that the taxonomy was imported into our database, we form the following SQLite query,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cf8460d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'my-first-taxonomy', 1), (2, 'my-first-taxonomy', 2)]\n"
     ]
    }
   ],
   "source": [
    "cursor = conn.execute(\"SELECT id,name,version FROM taxonomy\")\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15928078",
   "metadata": {},
   "source": [
    "Let us retrieve the 1st version of the taxonomy to inspect its node structure,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b29feb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tax = kdb.get_taxonomy(conn, taxonomy_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3818eee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown\n",
      "├── Anthro [Anthropogenic]\n",
      "│   └── Boat\n",
      "│       ├── Engine\n",
      "│       └── Prop [Propeller]\n",
      "└── Bio [Biological]\n",
      "    └── Whale\n",
      "        ├── HW [Humpback whale]\n",
      "        └── NARW [North Atlantic right whale]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tax.show(append_name=True)  #sound sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e12b862b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown\n",
      "├── GS [Gun shot]\n",
      "└── TC [Tonal call]\n",
      "    ├── FU [Faint Upcall]\n",
      "    └── LU [Loud Upcall]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tax.sound_types(\"NARW\").show(append_name=True)  #sound types for NARW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942a1a27",
   "metadata": {},
   "source": [
    "Finally, we remember to commit the changes and close the database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62d2624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debd84a5",
   "metadata": {},
   "source": [
    "## Adding an annotation job <a class=\"anchor\" id=\"adding-an-annotation-job\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822ce4bf",
   "metadata": {},
   "source": [
    "As always, we begin by reconnecting to the database,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "311b36a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(path_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cff910",
   "metadata": {},
   "source": [
    "The Korus schema can accomodate annotations jobs performed by humans as well as machines/algorithms. In this tutorial, however, we shall limit ourselves to human annotation jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0858df8",
   "metadata": {},
   "source": [
    "Let us inspect all the columns of the `job` table,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40f434e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('id', 'INTEGER'), ('taxonomy_id', 'INTEGER'), ('model_id', 'INTEGER'), ('annotator', 'TEXT'), ('primary_sound', 'JSON'), ('background_sound', 'JSON'), ('is_exhaustive', 'INTEGER'), ('configuration', 'JSON'), ('start_utc', 'TEXT'), ('end_utc', 'TEXT'), ('by_human', 'INTEGER'), ('by_machine', 'INTEGER'), ('issues', 'JSON'), ('comments', 'TEXT')]\n"
     ]
    }
   ],
   "source": [
    "cursor = conn.execute(\"SELECT name,type FROM PRAGMA_TABLE_INFO('job')\")\n",
    "columns = cursor.fetchall()\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c47746",
   "metadata": {},
   "source": [
    "For human-made annotations, you are advised to fill in the following fields,\n",
    "\n",
    " * `taxonomy_id`: The identifier of the taxonomy version used to annotate sounds\n",
    " * `primary_sound`: Sounds (source,type) that were systematically annotated\n",
    " * `background_sound`: Sounds that were annotated only opportunistically\n",
    " * `is_exhaustive`: Whether all primary sounds occurrences were annotated\n",
    " * `annotator`: The name or initials of the person who made the annotations\n",
    " * `start_utc`, `end_utc`: Time period during which the annotation job was performed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bb6a14",
   "metadata": {},
   "source": [
    "To be more concrete, let us assume that the acoustic analyst was tasked with annotating every occurrence of the (faint and loud) upcalls and gunshots made by the North Atlantic right whale, while only annotating anthropogenic sounds opportunistically. You would then specify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89496e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy_id = 1\n",
    "primary_sound = [(\"NARW\",\"FU\"), (\"NARW\",\"LU\"), (\"NARW\",\"GS\")]\n",
    "background_sound = (\"Anthro\",\"%\")  # % can be used as wildcard\n",
    "is_exhaustive = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc376e57",
   "metadata": {},
   "source": [
    "Let us now collect all the metadata in a dictionary and submit the entry to the database as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f96c3dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7d20809c5d40>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect all the data in a dict\n",
    "data = {\n",
    "    \"taxonomy_id\": taxonomy_id,\n",
    "    \"annotator\": \"AB\",\n",
    "    \"primary_sound\": primary_sound,\n",
    "    \"background_sound\": background_sound,\n",
    "    \"is_exhaustive\": is_exhaustive,\n",
    "    \"start_utc\": \"2022-10\",\n",
    "    \"end_utc\": \"2023-03\",\n",
    "    \"comments\": \"Vessel noise annotated opportunistically\"\n",
    "}\n",
    "\n",
    "# insert into the database\n",
    "kdb.insert_job(conn, values=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944be6a1",
   "metadata": {},
   "source": [
    "Before committing the changes and closing the connection to the database, let us add another annotation job. For this second job, we shall assume that the analyst was using version 2 of the taxonomy and only annotated particularly clear or nice examples of the NARW upcall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a2c7922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7d20809c6140>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect all the data in a dict\n",
    "data = {\n",
    "    \"taxonomy_id\": 2,\n",
    "    \"annotator\": \"CD\",\n",
    "    \"primary_sound\": (\"NARW\",\"Upcall\"),\n",
    "    \"is_exhaustive\": False,\n",
    "    \"start_utc\": \"2023-04-01\",\n",
    "    \"end_utc\": \"2023-04-07\",\n",
    "    \"comments\": \"Only annotated clear and nice upcalls\"\n",
    "}\n",
    "\n",
    "# insert into the database\n",
    "kdb.insert_job(conn, values=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d5bfe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e27506",
   "metadata": {},
   "source": [
    "## Linking files to jobs <a class=\"anchor\" id=\"linking-files-to-jobs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b86457",
   "metadata": {},
   "source": [
    "Having specified *who* made the annotations, *when* they were made, and (most importantly) *how* they were made, there is still one important piece of information missing, namely, *which* audio files were annotated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cad264",
   "metadata": {},
   "source": [
    "To retrieve the audio file IDs, we begin by querying the database for the relevant hydrophone deployment. Specifically, we search for deployments where `NS` or `Nova Scotia` appears in the location name and with data collected between 2019 and 2021. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5cc40367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-establish the connection to the database\n",
    "conn = sqlite3.connect(path_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d610332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2,)]\n"
     ]
    }
   ],
   "source": [
    "# Form SQLite query to search the database for deployments based on location name and time period\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        id\n",
    "    FROM \n",
    "        deployment \n",
    "    WHERE \n",
    "        location LIKE '%NS%'\n",
    "        OR location LIKE '%Nova Scotia%'\n",
    "        AND start_utc >= '2019'\n",
    "        AND end_utc <= '2021'\n",
    "\"\"\"\n",
    "c = conn.cursor()\n",
    "rows = c.execute(query).fetchall()\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db16953f",
   "metadata": {},
   "source": [
    "Our query returned one deployment matching the search criteria, with deployment ID = 2. Let us now retrieve the filenames and IDs of all the audio files associated with this deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1354dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('audio_20130623T080000.116Z.flac', 2), ('audio_20130623T083000.117Z.flac', 3), ('audio_20130624T080000.118Z.flac', 4)]\n"
     ]
    }
   ],
   "source": [
    "deployment_id = rows[0][0]\n",
    "\n",
    "query = f\"\"\"\n",
    "    SELECT \n",
    "        filename,id\n",
    "    FROM \n",
    "        file\n",
    "    WHERE \n",
    "        deployment_id = '{deployment_id}' \n",
    "\"\"\"\n",
    "rows = c.execute(query).fetchall()\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c356171",
   "metadata": {},
   "source": [
    "Finally, let us link these three audio files to first annotation job, i.e., the job performed by annotator `AB` in the previous section. Note that we only need the file IDs for this. The filenames were included in the query merely for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9fa6b56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# link files to job, one file at the time\n",
    "for row in rows:\n",
    "    file_id = row[1]\n",
    "    data = {\n",
    "        \"job_id\": 1, \n",
    "        \"file_id\": file_id,\n",
    "        \"channel\": 0  #recording channel (0,1,2,...)\n",
    "    }\n",
    "    kdb.insert_row(conn, table_name=\"file_job_relation\", values=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2a574e",
   "metadata": {},
   "source": [
    "Before we conclude this section, let us also link an audio file to the second annotation job,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fac35b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7d20809c6540>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"job_id\": 2, \n",
    "    \"file_id\": 1,\n",
    "    \"channel\": 0\n",
    "}\n",
    "kdb.insert_row(conn, table_name=\"file_job_relation\", values=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e14c14bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commit the changes and close the connection to the database\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c5c4b6",
   "metadata": {},
   "source": [
    "## Adding annotations <a class=\"anchor\" id=\"adding-annotations\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b14b4ab",
   "metadata": {},
   "source": [
    "We are now finally at the point where we can add annotations to the database. Although Korus contains functions for automatically ingesting annotation tables from CSV files or Excel spreadsheets, we shall first try to add annotations manually, as this will give us a better appreciation for the steps involved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597d948e",
   "metadata": {},
   "source": [
    "It is recommended to use the function `add_annotations` for adding annotations to the database. This function accepts a dictionary or a Pandas DataFrame as its input, with a specific set of keys or columns, as detailed in the Korus API Documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe59c51",
   "metadata": {},
   "source": [
    "In this example, we are given a CSV file with the two columns `filename` and `call_time`, specifying the names of the audio files and the within-file offsets (in seconds) of the acoustic signals identified by the acoustic analyst. \n",
    "\n",
    "We shall further assume that the analyst was using version 1 of the annotation taxonomy, and that all the entries in the CSV file are instances of the 'Loud Upcall' (LU) attribute to North Atlantic right whales (NARW). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d87ea8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           filename  call_time\n",
      "0   audio_20130623T080000.116Z.flac   1128.840\n",
      "1   audio_20130623T080000.116Z.flac   1153.526\n",
      "2   audio_20130623T080000.116Z.flac   1196.778\n",
      "3   audio_20130623T080000.116Z.flac   1227.642\n",
      "4   audio_20130623T080000.116Z.flac   1358.181\n",
      "5   audio_20130623T080000.116Z.flac   1437.482\n",
      "6   audio_20130623T080000.116Z.flac   1489.288\n",
      "7   audio_20130623T080000.116Z.flac   1511.670\n",
      "8   audio_20130623T080000.116Z.flac   1530.595\n",
      "9   audio_20130623T080000.116Z.flac   1536.580\n",
      "10  audio_20130623T080000.116Z.flac   1714.372\n",
      "11  audio_20130623T080000.116Z.flac   1768.251\n",
      "12  audio_20130623T080000.116Z.flac   1777.835\n",
      "13  audio_20130623T083000.117Z.flac     68.149\n",
      "14  audio_20130623T083000.117Z.flac    688.507\n",
      "15  audio_20130623T083000.117Z.flac    755.940\n",
      "16  audio_20130623T083000.117Z.flac    770.440\n",
      "17  audio_20130624T080000.118Z.flac     68.853\n",
      "18  audio_20130624T080000.118Z.flac    105.927\n",
      "19  audio_20130624T080000.118Z.flac   1057.015\n",
      "20  audio_20130624T080000.118Z.flac   1067.282\n",
      "21  audio_20130624T080000.118Z.flac   1290.563\n",
      "22  audio_20130624T080000.118Z.flac   1378.955\n",
      "23  audio_20130624T080000.118Z.flac   1428.648\n",
      "24  audio_20130624T080000.118Z.flac   1663.622\n",
      "25  audio_20130624T080000.118Z.flac   1676.682\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/annotations.csv\") #load data from CSV file into a Pandas DataFrame\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4704dae1",
   "metadata": {},
   "source": [
    "In order to submit these annotations to the database, we will need to look up the file identifiers. We will also need to assign each annotation a start time and a duration, in place of the `call_time` which corresponds roughly to the midpoint of the call. Since the duration is unknown, we will be using a fixed-sized window of 3.0 seconds, fully sufficient to capture the entire upcalls which typically are only 1 second long. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d74de",
   "metadata": {},
   "source": [
    "Let's begin by looking up the file identifiers and add them to the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35103ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['audio_20130623T080000.116Z.flac', 'audio_20130623T083000.117Z.flac', 'audio_20130624T080000.118Z.flac']\n"
     ]
    }
   ],
   "source": [
    "# list the unique filenames that appear in the annotation table\n",
    "filenames = df[\"filename\"].unique().tolist()\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c6f1502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-establish the connection to the database\n",
    "conn = sqlite3.connect(path_db)\n",
    "c = conn.cursor() # get cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a06dbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio_20130623T080000.116Z.flac': 2, 'audio_20130623T083000.117Z.flac': 3, 'audio_20130624T080000.118Z.flac': 4}\n"
     ]
    }
   ],
   "source": [
    "# query the 'file' table for files with the correct filename and retrieve their ID\n",
    "file_ids = dict()\n",
    "for filename in filenames:\n",
    "    query = f\"SELECT id FROM file WHERE filename = '{filename}'\"\n",
    "    file_id = c.execute(query).fetchall()[0][0]\n",
    "    file_ids[filename] = file_id # create a dictionary mapping: filename -> file_id\n",
    "    \n",
    "print(file_ids)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "090e24ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           filename  call_time  file_id\n",
      "0   audio_20130623T080000.116Z.flac   1128.840        2\n",
      "1   audio_20130623T080000.116Z.flac   1153.526        2\n",
      "2   audio_20130623T080000.116Z.flac   1196.778        2\n",
      "3   audio_20130623T080000.116Z.flac   1227.642        2\n",
      "4   audio_20130623T080000.116Z.flac   1358.181        2\n",
      "5   audio_20130623T080000.116Z.flac   1437.482        2\n",
      "6   audio_20130623T080000.116Z.flac   1489.288        2\n",
      "7   audio_20130623T080000.116Z.flac   1511.670        2\n",
      "8   audio_20130623T080000.116Z.flac   1530.595        2\n",
      "9   audio_20130623T080000.116Z.flac   1536.580        2\n",
      "10  audio_20130623T080000.116Z.flac   1714.372        2\n",
      "11  audio_20130623T080000.116Z.flac   1768.251        2\n",
      "12  audio_20130623T080000.116Z.flac   1777.835        2\n",
      "13  audio_20130623T083000.117Z.flac     68.149        3\n",
      "14  audio_20130623T083000.117Z.flac    688.507        3\n",
      "15  audio_20130623T083000.117Z.flac    755.940        3\n",
      "16  audio_20130623T083000.117Z.flac    770.440        3\n",
      "17  audio_20130624T080000.118Z.flac     68.853        4\n",
      "18  audio_20130624T080000.118Z.flac    105.927        4\n",
      "19  audio_20130624T080000.118Z.flac   1057.015        4\n",
      "20  audio_20130624T080000.118Z.flac   1067.282        4\n",
      "21  audio_20130624T080000.118Z.flac   1290.563        4\n",
      "22  audio_20130624T080000.118Z.flac   1378.955        4\n",
      "23  audio_20130624T080000.118Z.flac   1428.648        4\n",
      "24  audio_20130624T080000.118Z.flac   1663.622        4\n",
      "25  audio_20130624T080000.118Z.flac   1676.682        4\n"
     ]
    }
   ],
   "source": [
    "# finally, let's add a column to our data frame with the file IDs\n",
    "df[\"file_id\"] = df[\"filename\"].apply(lambda x: file_ids[x])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f84443",
   "metadata": {},
   "source": [
    "Now, let's assign start times and durations for every annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "275a5abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    file_id  start_ms  duration_ms granularity\n",
      "0         2   1127340         3000      window\n",
      "1         2   1152026         3000      window\n",
      "2         2   1195278         3000      window\n",
      "3         2   1226142         3000      window\n",
      "4         2   1356681         3000      window\n",
      "5         2   1435982         3000      window\n",
      "6         2   1487788         3000      window\n",
      "7         2   1510170         3000      window\n",
      "8         2   1529095         3000      window\n",
      "9         2   1535080         3000      window\n",
      "10        2   1712872         3000      window\n",
      "11        2   1766751         3000      window\n",
      "12        2   1776335         3000      window\n",
      "13        3     66649         3000      window\n",
      "14        3    687007         3000      window\n",
      "15        3    754440         3000      window\n",
      "16        3    768940         3000      window\n",
      "17        4     67353         3000      window\n",
      "18        4    104427         3000      window\n",
      "19        4   1055515         3000      window\n",
      "20        4   1065782         3000      window\n",
      "21        4   1289063         3000      window\n",
      "22        4   1377455         3000      window\n",
      "23        4   1427148         3000      window\n",
      "24        4   1662122         3000      window\n",
      "25        4   1675182         3000      window\n"
     ]
    }
   ],
   "source": [
    "# compute the annotation start time in milliseconds\n",
    "df[\"start_ms\"] = (df[\"call_time\"] - 1.5) * 1e3\n",
    "\n",
    "# set the duration to 3.0 seconds\n",
    "df[\"duration_ms\"] = 3000\n",
    "\n",
    "# cast both as integers (the type used for storing these quantities in the database)\n",
    "df[\"start_ms\"] = df[\"start_ms\"].astype(\"int\")\n",
    "df[\"duration_ms\"] = df[\"duration_ms\"].astype(\"int\")\n",
    "\n",
    "# drop the - now obsolete - filename and call_time columns\n",
    "df = df.drop(columns=[\"filename\",\"call_time\"])\n",
    "\n",
    "# specify the 'granularity' of each annotation\n",
    "df[\"granularity\"] = \"window\"\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d00e03c",
   "metadata": {},
   "source": [
    "Finally, we must add columns with the appropriate labels for the sound source and sound type,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "144d87a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sound_source\"] = \"NARW\"\n",
    "df[\"sound_type\"] = \"LU\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a1ad18",
   "metadata": {},
   "source": [
    "We are now ready to submit the annotations to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "052f6b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_ids = kdb.add_annotations(conn, annot_tbl=df, job_id=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1f8dd7",
   "metadata": {},
   "source": [
    "Note that the `add_annotations` returns the indices of the annotations just added. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96897b3d",
   "metadata": {},
   "source": [
    "Another useful feature of Korus is the `add_negatives` function, which automatically generates annotations for the 'quiet' periods, i.e., periods during which no 'primary sounds' were heard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb62f22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_ids = kdb.add_negatives(conn, job_id=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8032a1c",
   "metadata": {},
   "source": [
    "Content that the annotations have been inserted into our database, we commit the changes and close the connection,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ecd29bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc34a09",
   "metadata": {},
   "source": [
    "# Customization <a class=\"anchor\" id=\"customization\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52b88f1",
   "metadata": {},
   "source": [
    "It is not recommended to remove existing columns or tables from the Korus database as this may compromise core functionalities of the Korus API. However, you are more than welcome to extend the database schema. To do so, you will have to interact directly with the database using SQLite syntax. For example, adding a column to an existing table is as easy as,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2e14297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a connection to the database\n",
    "conn = sqlite3.connect(path_db)\n",
    "\n",
    "# add a column named 'hydrophone_color' to the deployment table with default value 'yellow'\n",
    "conn.execute(\"ALTER TABLE deployment ADD COLUMN hydrophone_color TEXT DEFAULT 'yellow'\")\n",
    "\n",
    "# commit changes and close\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
